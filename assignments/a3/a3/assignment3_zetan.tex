%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%Assignment 2
%Author Zetan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{multirow}

\usepackage{url}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Python} % Load python syntax for listings, for a list of other languagesftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf supported see: 
\lstset{
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % python functions bold and blue
        keywordstyle=[2]\color{Purple}, % python function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        breaklines=true,
        %
        % Put standard python functions not included in the default language here
        morekeywords={rand},
        %
        % Put python function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a pyton script, the first parameter is the filename of the script (without .py), the second parameter is the caption
\newcommand{\pythonscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[language=python,caption=#2,label=#1]{#1.py}
\end{itemize}
}
% Creates a new command to include a shell script, the first parameter is the filename of the script (without .sh), the second parameter is the caption
\newcommand{\shellscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[language=bash,caption=#2,label=#1]{#1.sh}
\end{itemize}
}
% Creates a new command to include a R script, the first parameter is the filename of the script (without .R), the second parameter is the caption
\newcommand{\Rscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[language=R,caption=#2,label=#1]{#1.R}
\end{itemize}
}
%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#3} % Assignment title
\newcommand{\hmwkDueDate}{Thursday,\ February\ 18,\ 2016} % Due date
\newcommand{\hmwkClass}{Web Science\ cs532} % Course/class
\newcommand{\hmwkClassTime}{4:20pm} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Dr.Michael.L.Nelson} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Zetan Li} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}
Download the 1000 URIs from assignment \#2.  ``curl",``wget", or
``lynx" are all good candidate programs to use.  We want just the
raw HTML, not the images, stylesheets, etc.\\
\\
from the command line:\\
\\
\% curl \url{http://www.cnn.com/ > www.cnn.com}\\
\\
\% wget -O \url{www.cnn.com} \url{http://www.cnn.com/}\\
\\
\% lynx -source \url{ http://www.cnn.com/ > www.cnn.com}\\
\\
``www.cnn.com" is just an example output file name, keep in mind
that the shell will not like some of the characters that can occur
in URIs (e.g., ``?", ``\&").  You might want to hash the URIs, like:\\
\\
\% echo -n ``\url{http://www.cs.odu.edu/show\_features.shtml?72}" $\vert$ md5
41d5f125d13b4bb554e6e31b6b591eeb\\
\\
(``md5sum" on some machines; note the ``-n" in echo -- this removes
the trailing newline.) \\
\\
Now use a tool to remove (most) of the HTML markup.  ``lynx" will
do a fair job:\\
\\
\% lynx -dump -force\_html \url{www.cnn.com > www.cnn.com.processed}\\
\\
Use another (better) tool if you know of one.  Keep both files 
for each URI (i.e., raw HTML and processed). \\
\centerline{SOLUTION}
Copy the url list from assignment 2 and run shell script blow to download web page from every url in the list.
\shellscript{downuri}{Shell script for web downloading}
The use lynx in description above to remove all the html tags.
\pagebreak
\shellscript{makehtml}{Shell script to convert raw web page}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%	PROBLEM 2
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
Choose a query term (e.g., ``shadow") that is not a stop word
(see week 5 slides) and not HTML markup from step 1 (e.g., ``http")
that matches at least 10 documents (hint: use ``grep" on the processed
files).  If the term is present in more than 10 documents, choose
any 10 from your list.  (If you do not end up with a list of 10
URIs, you've done something wrong).\\
\\
As per the example in the week 5 slides, compute TFIDF values for
the term in each of the 10 documents and create a table with the
TF, IDF, and TFIDF values, as well as the corresponding URIs.  The
URIs will be ranked in decreasing order by TFIDF values.  For
example:\\
\\
Table 1. 10 Hits for the term ``shadow", ranked by TFIDF.\\
\\
\texttt {TFIDF	TF	IDF	URI}\\
\texttt {-----	--	---	---}\\
\texttt{0.150	0.014	10.680	http://foo.com/}\\
\texttt{0.044	0.008	 5.510	http://bar.com/}\\
\\

You can use Google or Bing for the DF estimation.  To count the
number of words in the processed document (i.e., the deonminator
for TF), you can use ``wc":\\
\\
\% wc -w www.cnn.com.processed\\
    2370 www.cnn.com.processed
\\
It won't be completely accurate, but it will be probably be
consistently inaccurate across all files.  You can use more 
accurate methods if you'd like.  \\
\\
Don't forget the log base 2 for IDF, and mind your significant
digits!\\
\centerline{SOLUTION}
To get IDF, we should search on the google to get the total amount of file contain the term we required. Here, we choose the term ``game".\\
*Note: The data we use is from Feb 16, and the image we captured is in Feb 17, You can see the result increased.\\
\pagebreak
\begin{figure}[h]
\centering
\caption{The google search result for ``game''}
\includegraphics[width=5in]{googlesearch}
\end{figure}
Then search hard for total amount of web site indexed in google, and get the answer\cite{totalweb}\\
So now we have DF and total number of documents to calculate IDF.\\
Next, grep each file in the folder count the term using wc, and calculate the normalized TF.\\
During calculating, we use bc to enable float calculation in bash, and awk to do the log2 work. In order to get 10 different domain for problem 3, we listed 20 urls.\\
Finally multiply them together to get TF-IDF, then use sort to get ranked list.\\
\shellscript{searchfile}{Code to get TF and IDF}
\newpage
\begin{table}[!hbp]
\caption{The table of TF value of top ten url}
\centering
\hspace*{-10mm}\begin{tabular}{|c|c|c|c|}
\hline
TF-IDF & TF & IDF & URI\\
\hline
\multirow{2}{*}{8.589015}&\multirow{2}{*}{0.666666}&\multirow{2}{*}{12.883536}&http://mmanor.17bullets.com/twitter\_post.php?messageId=quest\\
&&&\&values=quests.2584.title\&tw\_locale=en\_US\%0A\\
\hline
1.004542&0.077971&12.883536&http://games-fors.ru/top/art/edge-of-tomorrow-game/\%0A\\
\hline
0.460122&0.035714&12.883536&http://www.freem.ne.jp/win/game/9914\%0A\\
\hline
\multirow{3}{*}{0.080303}&\multirow{3}{*}{0.006233}&\multirow{3}{*}{12.883536}&http://www.vegascoverage.com/2016/02/08/quinnipiac-vs-saint\\
&&&-peters-ncaa-sports-betting-odds-pick-and-prediction/?utm\_campaign=\\
&&&Vegas+Coverage\&utm\_source=twitterfeed\&utm\_medium=twitter\\
\hline
0.068527&0.005319&12.883536&https://www.facebook.com/jaycecabell1/posts/1645995465662075\\
\hline
\multirow{2}{*}{0.067728}&\multirow{2}{*}{0.005257}&\multirow{2}{*}{12.883536}&http://www.ebay.com/itm/like/331772926586?item=331772926586\\
&&&\&lgeo=1\&utm\_medium=twitter\&utm\_source=dlvr.it\&vectorid=229466\&rmvSB=true\\
\hline
0.059637&0.004629&12.883536&http://d-comic.net/movie\_view.php?id=35529\\
\hline
0.059019&0.004581&12.883536&https://www.youtube.com/watch?v=rNwWpIRX7Pw\&feature=youtu.be\&aBEST\%0A\\
\hline
.052152&.004048&12.883536&http://endzoneblog.com/denvers-defense-overwhelms-carolina-super-bowl-50/\\
\hline
\multirow{3}{*}{0.048068}&\multirow{3}{*}{0.003731}&\multirow{3}{*}{12.883536}&http://PETERSUN19.rsscb.com/asapin/datingtothebank/02081606/\\
&&&the-love-game-how-to-date-the-most-beautiful-women-you-\\
&&&ever-wanted.php?utm\_source=dlvr.it\&utm\_medium=twitter\\
\hline
\end{tabular}
\end{table}
\end{homeworkProblem}

%----------------------------------------------------------------------------------------
%&PROBLEM 3
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}
Now rank the same 10 URIs from question \#2, but this time 
by their PageRank.  Use any of the free PR estimaters on the web,
such as:\\
\\
\url{http://www.prchecker.info/check_page_rank.php}\\
\url{http://www.seocentro.com/tools/search-engines/pagerank.html}\\
\url{http://www.checkpagerank.net/}\\
\\
If you use these tools, you'll have to do so by hand (they have
anti-bot captchas), but there is only 10.  Normalize the values
they give you to be from 0 to 1.0.  Use the same tool on all 10
(again, consistency is more important than accuracy).\\
\\
Create a table similar to Table 1:\\
\\
Table 2.  10 hits for the term ``shadow", ranked by PageRank.\\
\\
PageRank	URI\\
\texttt{--------	---}\\
0.9		http://bar.com/\\
0.5		http://foo.com/\\
\\
Briefly compare and contrast the rankings produced in questions 2
and 3.\\
\\
\centerline{SOLUTION}
\\
We choose \url{http://www.seocentro.com/tools/search-engines/pagerank.html} for the tool for this problem. As all the user-specific page will result in ``undefined" from search, we use the domain as PR search term instead (not accurate but this is one way to solve the undef problem). \\
But still, some of the domain result in a n/a answer.
\begin{table}[!hbp]
\caption{Page rank from website estimation}
\centering
\begin{tabular}{|c|c|}
\hline
URI&Page Rank\\
\hline
http://mmanor.17bullets.com&N/A\\
\hline
http://games-fors.ru&N/A\\
\hline
http://www.freem.ne.jp&5/10\\
\hline
http://www.vegascoverage.com&N/A\\
\hline
https://www.facebook.com&9/10\\
\hline
http://www.ebay.com&8/10\\
\hline
http://d-comic.net&1/10\\
\hline
https://www.youtube.com&9/10\\
\hline
http://endzoneblog.com&N/A\\
\hline
http://PETERSUN19.rsscb.com&N/A\\
\hline
\end{tabular}
\end{table}

\begin{table}[!hbp]
\caption{Normalized page rank}
\centering
\begin{tabular}{|c|c|}
\hline
URI&Page Rank\\
\hline
http://mmanor.17bullets.com&0\\
\hline
http://games-fors.ru&0\\
\hline
http://www.freem.ne.jp&0.5\\
\hline
http://www.vegascoverage.com&0\\
\hline
https://www.facebook.com&0.9\\
\hline
http://www.ebay.com&0.8\\
\hline
http://d-comic.net&0.1\\
\hline
https://www.youtube.com&0.9\\
\hline
http://endzoneblog.com&0\\
\hline
http://PETERSUN19.rsscb.com&0\\
\hline
\end{tabular}
\end{table}

\begin{table}[!hbp]
\caption{Page rank table}
\centering
\begin{tabular}{|c|c|}
\hline
Page Rank&URI\\
\hline
0.9&https://www.facebook.com\\
\hline
0.9&https://www.youtube.com\\
\hline
0.8&http://www.ebay.com\\
\hline
0.5&http://www.freem.ne.jp\\
\hline
0.1&http://d-comic.net\\
\hline
0&http://mmanor.17bullets.com\\
\hline
0&http://games-fors.ru\\
\hline
0&http://www.vegascoverage.com\\
\hline
0&http://endzoneblog.com\\
\hline
0&http://PETERSUN19.rsscb.com\\
\hline
\end{tabular}
\end{table}
\textbf{Compare}\\
Because we use the domain name for page rank, so the accuracy is not so good. A domain may be popular but certain user page may not well-known. Additionally, as we can see, the facebook and youtube has high page rank, just because many websites have share link point to them, but if we point to certain video or post in their site, like the page contain ``game'' we picked from our own web database, they do not have so much link refer to them, or even none, because our web tool cannot even find them, but they are more relevant to our search term(e.g the top 1 ranked web site in our selection, though with zero page rank, but it is really a game site).
\end{homeworkProblem}

\bibliographystyle{plain}
\bibliography{ref}
\end{document}